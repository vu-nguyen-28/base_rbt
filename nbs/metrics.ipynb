{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics\n",
    "\n",
    "> Metrics (e.g. ROC) and also a `predict_model` function that is more efficient than FastAI defaults when the dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions given xval and yval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from statistics import mean,stdev\n",
    "import numpy as np\n",
    "import scikitplot \n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score,precision_recall_curve,roc_curve,auc,classification_report,confusion_matrix\n",
    "from scipy import interp\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_model(xval,yval,model,aug_pipelines_test,numavg=3,criterion = CrossEntropyLossFlat(),deterministic=False):\n",
    "    \"Note that this assumes xval is entire validation set. If it doesn't fit in memory, can't use this guy\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    N=xval.shape[0]\n",
    "    \n",
    "    if not deterministic:\n",
    "\n",
    "        probs=0\n",
    "        for _ in range(numavg):\n",
    "\n",
    "            probs += torch.softmax(model(aug_pipelines_test[0](xval)),dim=1) #test time augmentation. This also gets around issue of randomness in the dataloader in each session...\n",
    "\n",
    "        probs *= 1/numavg\n",
    "        \n",
    "    else:\n",
    "        probs = torch.softmax(model(xval),dim=1)\n",
    "\n",
    "    \n",
    "    ypred = cast(torch.argmax(probs, dim=1),TensorCategory)\n",
    "\n",
    "    correct = (ypred == yval)#.type(torch.FloatTensor)\n",
    "\n",
    "    #correct = (torch.argmax(ypred,dim=1) == yval).type(torch.FloatTensor)\n",
    "    num_correct = correct.sum()\n",
    "    accuracy = num_correct/N\n",
    "\n",
    "    #val_loss = criterion(scores,yval)\n",
    "    \n",
    "    return probs,ypred,accuracy.item()#,val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def predict_ensemble(yval,scores1,scores2):\n",
    "    \"scores can be normalized (softmax) or not\"\n",
    "\n",
    "    N=yval.shape[0]\n",
    "\n",
    "    scores = 0.5*scores1 + 0.5*scores2\n",
    "\n",
    "    ypred = cast(torch.argmax(scores, dim=1),TensorCategory)\n",
    "\n",
    "    correct = (ypred == yval)#.type(torch.FloatTensor)\n",
    "\n",
    "    #correct = (torch.argmax(ypred,dim=1) == yval).type(torch.FloatTensor)\n",
    "    num_correct = correct.sum()\n",
    "    accuracy = num_correct/N\n",
    "    \n",
    "    return ypred,accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def classification_report_wrapper(ypred, y, int_to_classes, print_report=True):\n",
    "    # Convert ypred and y to numpy arrays\n",
    "    ypred = ypred.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "\n",
    "    # Get the unique classes in y\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # Create labels from unique_y and vocab\n",
    "    labels = [int_to_classes[i] for i in unique_y]\n",
    "\n",
    "    # Get the classification report as a dictionary\n",
    "    report = classification_report(y, ypred, target_names=labels, output_dict=True)\n",
    "\n",
    "    if print_report:\n",
    "        print(classification_report(y, ypred, target_names=labels))\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Dog       1.00      0.67      0.80         3\n",
      "         Cat       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.75      0.83      0.73         4\n",
      "weighted avg       0.88      0.75      0.77         4\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Dog       0.75      1.00      0.86         3\n",
      "         Cat       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.38      0.50      0.43         4\n",
      "weighted avg       0.56      0.75      0.64         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "ypred=np.array([0,1,0,1])\n",
    "ypred2=np.array([0,0,0,0])\n",
    "y=np.array([0,0,0,1])\n",
    "int_to_classes={0:'Dog',1:'Cat'}\n",
    "labels = ['Dog','Cat']\n",
    "#classification_report_wrapper(ypred, y, int_to_classes)\n",
    "\n",
    "report = classification_report(y, ypred, target_names=labels,output_dict=True)\n",
    "report2 = classification_report(y, ypred2, target_names=labels,output_dict=True)\n",
    "print(classification_report(y, ypred, target_names=labels))\n",
    "print(classification_report(y, ypred2, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#This code comes from https://gist.github.com/geblanco/5cfe4a3224e021113968a8c7ebe31419\n",
    "def format_classification_report(data_dict):\n",
    "\n",
    "\n",
    "    non_label_keys = [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "    y_type = \"binary\"\n",
    "    digits = 2\n",
    "\n",
    "    target_names = [\n",
    "        \"%s\" % key for key in data_dict.keys() if key not in non_label_keys\n",
    "    ]\n",
    "\n",
    "    # labelled micro average\n",
    "    micro_is_accuracy = (y_type == \"multiclass\" or y_type == \"binary\")\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    p = [data_dict[l][headers[0]] for l in target_names]\n",
    "    r = [data_dict[l][headers[1]] for l in target_names]\n",
    "    f1 = [data_dict[l][headers[2]] for l in target_names]\n",
    "    s = [data_dict[l][headers[3]] for l in target_names]\n",
    "\n",
    "    rows = zip(target_names, p, r, f1, s)\n",
    "\n",
    "    if y_type.startswith(\"multilabel\"):\n",
    "        average_options = (\"micro\", \"macro\", \"weighted\", \"samples\")\n",
    "    else:\n",
    "        average_options = (\"micro\", \"macro\", \"weighted\")\n",
    "\n",
    "    longest_last_line_heading = \"weighted avg\"\n",
    "    name_width = max(len(cn) for cn in target_names)\n",
    "    width = max(name_width, len(longest_last_line_heading), digits)\n",
    "    head_fmt = \"{:>{width}s} \" + \" {:>9}\" * len(headers)\n",
    "    report = head_fmt.format(\"\", *headers, width=width)\n",
    "    report += \"\\n\\n\"\n",
    "    row_fmt = \"{:>{width}s} \" + \" {:>9.{digits}f}\" * 3 + \" {:>9}\\n\"\n",
    "    for row in rows:\n",
    "        report += row_fmt.format(*row, width=width, digits=digits)\n",
    "    report += \"\\n\"\n",
    "\n",
    "    # compute all applicable averages\n",
    "    for average in average_options:\n",
    "        if average.startswith(\"micro\") and micro_is_accuracy:\n",
    "            line_heading = \"accuracy\"\n",
    "        else:\n",
    "            line_heading = average + \" avg\"\n",
    "\n",
    "        if line_heading == \"accuracy\":\n",
    "            avg = [data_dict[line_heading], sum(s)]\n",
    "            row_fmt_accuracy = \"{:>{width}s} \" + \\\n",
    "                    \" {:>9.{digits}}\" * 2 + \" {:>9.{digits}f}\" + \\\n",
    "                    \" {:>9}\\n\"\n",
    "            report += row_fmt_accuracy.format(line_heading, \"\", \"\",\n",
    "                                              *avg, width=width,\n",
    "                                              digits=digits)\n",
    "        else:\n",
    "            avg = list(data_dict[line_heading].values())\n",
    "            report += row_fmt.format(line_heading, *avg,\n",
    "                                     width=width, digits=digits)\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def Mean_Report(reports, classes):\n",
    "    N = len(reports)\n",
    "    mean_report = {}\n",
    "    for k in reports[0].keys():\n",
    "        if k!='accuracy':\n",
    "            mean_report[k] = {}\n",
    "            for metric in reports[0][k].keys():\n",
    "                att = 0\n",
    "                for _report in reports:\n",
    "                    att += _report[k][metric]\n",
    "                mean_report[k][metric] = att / N\n",
    "        else:\n",
    "            att = 0\n",
    "            for _report in reports:\n",
    "                att += _report[k]\n",
    "                \n",
    "            mean_report[k] = att / N\n",
    "    return mean_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def Mean_Results(results,vocab):\n",
    "    \"Get mean classif report and display it\"\n",
    "\n",
    "    lst = list(vocab) + ['accuracy', 'macro avg', 'weighted avg']\n",
    "    reports=[]\n",
    "    accs=[]\n",
    "    for i in results.keys():\n",
    "        if type(i)!=int:\n",
    "            continue\n",
    "        report = {j:results[i][j] for j in results[i].keys() if j in lst}\n",
    "        reports.append(report)\n",
    "        accs.append(results[i]['acc'])\n",
    "    mean_report = Mean_Report(reports,vocab)\n",
    "    print(format_classification_report(mean_report))\n",
    "    \n",
    "    print(f'mean acc is {mean(accs)} with std {stdev(accs)}')\n",
    "\n",
    "    return mean_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# _mean_report = Mean_Report([report,report2],vocab=['Dog','Cat'])\n",
    "\n",
    "# test_eq(_mean_report.keys(),report.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# report['acc']=0.01\n",
    "# report2['acc']=0.02\n",
    "# Reports = {0:report,1:report2}\n",
    "# Mean_Results(Reports,vocab=['Dog','Cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#Verbose test to make sure it is working\n",
    "# for k in report.keys():\n",
    "    \n",
    "#     if k!='accuracy':\n",
    "        \n",
    "#         for metric in report[k].keys():\n",
    "\n",
    "            \n",
    "#             diff = abs(_mean_report[k][metric] - 0.5*(report[k][metric]+report2[k][metric]))\n",
    "#             #assert diff<1e-7\n",
    "            \n",
    "    \n",
    "#     if k == 'accuracy':\n",
    "        \n",
    "#         diff = abs(_mean_report[k]-0.5*(report[k]+report2[k]))\n",
    "#         #test_eq(_mean_report[k],0.5*(report[k]+report2[k]))\n",
    "        \n",
    "    \n",
    "#     assert diff<1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#print(format_classification_report(_mean_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#print(format_classification_report(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#print(format_classification_report(report2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def print_confusion_matrix(ypred, y, vocab):\n",
    "    # Convert ypred and y to numpy arrays\n",
    "    ypred = ypred.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    \n",
    "    # Get the class labels from vocab\n",
    "    labels = [vocab[i] for i in range(len(vocab))]\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y, ypred)\n",
    "    \n",
    "    # Create a DataFrame from the confusion matrix\n",
    "    df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
    "    \n",
    "    # Use seaborn to create a heatmap of the confusion matrix with blue and white colors\n",
    "    sns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We monkey patch some plotting functions from scikitplot and edit them - we need greater control of legend etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _plot_precision_recall(y_true, y_probas,\n",
    "                          title='Precision-Recall Curve',\n",
    "                          plot_micro=True,\n",
    "                          classes_to_plot=None, ax=None,\n",
    "                          figsize=None, cmap='nipy_spectral',\n",
    "                          title_fontsize=\"large\",\n",
    "                          text_fontsize=\"small\"):\n",
    "    \"\"\"Generates the Precision Recall Curve from labels and probabilities. This is moneky patched from: \n",
    "    https://github.com/reiinakano/scikit-plot/blob/26007fbf9f05e915bd0f6acb86850b01b00944cf/scikitplot/metrics.py\n",
    "\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_probas = np.array(y_probas)\n",
    "\n",
    "    classes = np.unique(y_true)\n",
    "    probas = y_probas\n",
    "\n",
    "    if classes_to_plot is None:\n",
    "        classes_to_plot = classes\n",
    "\n",
    "    binarized_y_true = label_binarize(y_true, classes=classes)\n",
    "    if len(classes) == 2:\n",
    "        binarized_y_true = np.hstack(\n",
    "            (1 - binarized_y_true, binarized_y_true))\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.set_title(title, fontsize=title_fontsize)\n",
    "\n",
    "    indices_to_plot = np.in1d(classes, classes_to_plot)\n",
    "    for i, to_plot in enumerate(indices_to_plot):\n",
    "        if to_plot:\n",
    "            average_precision = average_precision_score(\n",
    "                binarized_y_true[:, i],\n",
    "                probas[:, i])\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                y_true, probas[:, i], pos_label=classes[i])\n",
    "            color = plt.cm.get_cmap(cmap)(float(i) / len(classes))\n",
    "            ax.plot(recall, precision, lw=2,\n",
    "                    label='{0} '\n",
    "                          '(area = {1:0.3f})'.format(classes[i],\n",
    "                                                     average_precision),\n",
    "                    color=color)\n",
    "\n",
    "    if plot_micro:\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            binarized_y_true.ravel(), probas.ravel())\n",
    "        average_precision = average_precision_score(binarized_y_true,\n",
    "                                                    probas,\n",
    "                                                    average='micro')\n",
    "        ax.plot(recall, precision,\n",
    "                label='micro-avg '\n",
    "                      '(area = {0:0.3f})'.format(average_precision),\n",
    "                color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc='lower left', fontsize=text_fontsize)\n",
    "    return ax\n",
    "\n",
    "def _plot_roc(y_true, y_probas, title='ROC Curves',\n",
    "                   plot_micro=True, plot_macro=True, classes_to_plot=None,\n",
    "                   ax=None, figsize=None, cmap='nipy_spectral',\n",
    "                   title_fontsize=\"large\", text_fontsize=\"medium\"):\n",
    "    \"\"\"Generates the ROC curves from labels and predicted scores/probabilities. Monkey patched like above function.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_probas = np.array(y_probas)\n",
    "\n",
    "    classes = np.unique(y_true)\n",
    "    probas = y_probas\n",
    "\n",
    "    if classes_to_plot is None:\n",
    "        classes_to_plot = classes\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.set_title(title, fontsize=title_fontsize)\n",
    "\n",
    "    fpr_dict = dict()\n",
    "    tpr_dict = dict()\n",
    "\n",
    "    indices_to_plot = np.in1d(classes, classes_to_plot)\n",
    "    for i, to_plot in enumerate(indices_to_plot):\n",
    "        fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true, probas[:, i],\n",
    "                                                pos_label=classes[i])\n",
    "        if to_plot:\n",
    "            roc_auc = auc(fpr_dict[i], tpr_dict[i])\n",
    "            color = plt.cm.get_cmap(cmap)(float(i) / len(classes))\n",
    "            ax.plot(fpr_dict[i], tpr_dict[i], lw=2, color=color,\n",
    "                    label='{0} (area = {1:0.2f})'\n",
    "                          ''.format(classes[i], roc_auc))\n",
    "\n",
    "    if plot_micro:\n",
    "        binarized_y_true = label_binarize(y_true, classes=classes)\n",
    "        if len(classes) == 2:\n",
    "            binarized_y_true = np.hstack(\n",
    "                (1 - binarized_y_true, binarized_y_true))\n",
    "        fpr, tpr, _ = roc_curve(binarized_y_true.ravel(), probas.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr,\n",
    "                label='micro-avg'\n",
    "                      '(area = {0:0.2f})'.format(roc_auc),\n",
    "                color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    if plot_macro:\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr_dict[x] for x in range(len(classes))]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(len(classes)):\n",
    "            mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= len(classes)\n",
    "        roc_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "        ax.plot(all_fpr, mean_tpr,\n",
    "                label='macro-avg'\n",
    "                      '(area = {0:0.2f})'.format(roc_auc),\n",
    "                color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=text_fontsize)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=text_fontsize)\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc='lower right', fontsize=text_fontsize)\n",
    "    return ax\n",
    "\n",
    "#| export\n",
    "\n",
    "def plot_roc(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "\n",
    "    #scikitplot.metrics.plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "    _plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "\n",
    "def plot_pr(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "\n",
    "    #scikitplot.metrics.plot_precision_recall(_ytest, probs)#,plot_micro=True,plot_macro=False)\n",
    "    _plot_precision_recall(_ytest, probs,plot_micro=True)\n",
    "\n",
    "    plt.legend(loc='best', fontsize='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to plot ROC curve and precision-recall curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def plot_roc(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "    \n",
    "    #scikitplot.metrics.plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "    _plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "\n",
    "def plot_pr(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "    \n",
    "    #scikitplot.metrics.plot_precision_recall(_ytest, probs)#,plot_micro=True,plot_macro=False)\n",
    "    _plot_precision_recall(_ytest, probs,plot_micro=True)\n",
    "\n",
    "    plt.legend(loc='best', fontsize='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def Auc_Dict(ytest,probs,int_to_classes=None):\n",
    "    \"Mostly used to verify results of plot (debug)\"\n",
    "\n",
    "    # # define your ytest and probs variables\n",
    "    # ytest = ['cat', 'dog', 'bear', 'dog', 'cat']\n",
    "    # probs = np.array([[0.7, 0.2, 0.1], [0.3, 0.6, 0.1], [0.2, 0.3, 0.5], [0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])\n",
    "\n",
    "    # calculate AUC for each class\n",
    "    auc_dict = {}\n",
    "    for i, cls in enumerate(np.unique(ytest)):\n",
    "        y_true = np.array([1 if y == cls else 0 for y in ytest]) # one-vs-all labels\n",
    "        y_score = probs[:, i] # get probability for current class\n",
    "        auc = roc_auc_score(y_true, y_score)#, multi_class='ovr')\n",
    "        auc_dict[cls] = auc\n",
    "\n",
    "    if int_to_classes!=None:\n",
    "        auc_dict = {int_to_classes[i]:auc_dict[i] for i in auc_dict}\n",
    "    \n",
    "    return auc_dict\n",
    "\n",
    "\n",
    "def Pr_Dict(ytest,probs,int_to_classes=None):\n",
    "    \"Mostly used to verify results of plot (debug)\"\n",
    "\n",
    "    # # define your ytest and probs variables\n",
    "    # ytest = ['cat', 'dog', 'bear', 'dog', 'cat']\n",
    "    # probs = np.array([[0.7, 0.2, 0.1], [0.3, 0.6, 0.1], [0.2, 0.3, 0.5], [0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])\n",
    "\n",
    "    # calculate AUC for each class\n",
    "    pr_dict = {}\n",
    "    for i, cls in enumerate(np.unique(ytest)):\n",
    "        y_true = np.array([1 if y == cls else 0 for y in ytest]) # one-vs-all labels\n",
    "        y_score = probs[:, i] # get probability for current class\n",
    "        pr = average_precision_score(y_true, y_score)#, multi_class='ovr')\n",
    "        pr_dict[cls] = pr\n",
    "\n",
    "    if int_to_classes!=None:\n",
    "        pr_dict = {int_to_classes[i]:pr_dict[i] for i in pr_dict}\n",
    "    \n",
    "    return pr_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_whole_model(dls_test, model, aug_pipelines_test, numavg=3, criterion=CrossEntropyLossFlat(), deterministic=False):\n",
    "    \"\"\"\n",
    "    Predicts the labels and probabilities for the entire test set using the specified model and data augmentation\n",
    "    pipelines. Returns a dictionary containing the labels, probabilities, predicted labels, and accuracy.\n",
    "\n",
    "    Args:\n",
    "        dls_test: The test dataloader.\n",
    "        model: The trained model.\n",
    "        aug_pipelines_test: The test data augmentation pipelines.\n",
    "        numavg: The number of times to perform test-time augmentation.\n",
    "        criterion: The loss function to use for computing the accuracy.\n",
    "        deterministic: Whether to use deterministic computation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the labels, probabilities, predicted labels, and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_len = len(dls_test.dataset)\n",
    "    y = torch.zeros(total_len, dtype=torch.long)\n",
    "    probs = torch.zeros(total_len, model.head.out_features)\n",
    "    ypred = torch.zeros(total_len, dtype=torch.long)\n",
    "\n",
    "    start_idx = 0\n",
    "    for xval, yval in dls_test.train:\n",
    "        end_idx = start_idx + len(xval)\n",
    "        _probs, _ypred, acc = predict_model(xval, yval, model, aug_pipelines_test, numavg, criterion, deterministic)\n",
    "        y[start_idx:end_idx] = yval\n",
    "        probs[start_idx:end_idx] = _probs\n",
    "        ypred[start_idx:end_idx] = _ypred\n",
    "        start_idx = end_idx\n",
    "\n",
    "    # Calculate the overall accuracy\n",
    "    acc = (ypred == y).float().mean().item()\n",
    "\n",
    "    # Return the predictions and labels in a dictionary\n",
    "    #return {'y': y, 'probs': probs, 'ypred': ypred, 'acc': acc}\n",
    "    return y,probs,ypred,acc\n",
    "\n",
    "def get_dls_metrics(dls,model,aug_pipelines_test,int_to_classes): #note that we can't call dls.vocab as it might be smaller on the test set\n",
    "    \"get metrics from model and dataloader\"\n",
    "\n",
    "    ytest,probs,preds,Acc = predict_whole_model(dls,model,aug_pipelines_test,numavg=3)\n",
    "    metrics = classification_report_wrapper(preds, ytest,int_to_classes, print_report=True)\n",
    "    \n",
    "    plot_roc(ytest,probs,int_to_classes)\n",
    "    auc_dict = Auc_Dict(ytest,probs,int_to_classes)\n",
    "    print(f'auc_dict is: {auc_dict}')\n",
    "    plot_pr(ytest,probs,int_to_classes)\n",
    "    pr_dict = Pr_Dict(ytest,probs,int_to_classes)\n",
    "    print(f'auc_dict is: {pr_dict}')\n",
    "\n",
    "    metrics['ytest']=ytest\n",
    "    metrics['probs']=probs\n",
    "    metrics['preds']=preds\n",
    "    metrics['acc']=Acc\n",
    "    metrics['auc_dict']=auc_dict\n",
    "    metrics['pr_dict']=pr_dict\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_xval_metrics(xval,yval,model,aug_pipelines_test,int_to_classes,numavg=3): #note that we can't call dls.vocab as it might be smaller on the test set\n",
    "    \"get metrics from gives batch (xval,yval)\"\n",
    "\n",
    "    probs,preds,Acc = predict_model(xval,yval,model,aug_pipelines_test,numavg=3)\n",
    "    metrics = classification_report_wrapper(preds, yval,int_to_classes, print_report=True)\n",
    "    metrics['acc']=Acc\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def Mean_Results(results,vocab):\n",
    "    \"Get mean classif report and display it\"\n",
    "\n",
    "    lst = list(vocab) + ['accuracy', 'macro avg', 'weighted avg']\n",
    "    reports=[]\n",
    "    accs=[]\n",
    "    for i in results.keys():\n",
    "        if type(i)!=int:\n",
    "            continue\n",
    "        report = {j:results[i][j] for j in results[i].keys() if j in lst}\n",
    "        reports.append(report)\n",
    "        accs.append(results[i]['acc'])\n",
    "    mean_report = Mean_Report(reports,vocab)\n",
    "    print(format_classification_report(mean_report))\n",
    "    \n",
    "    print(f'mean acc is {mean(accs)} with std {stdev(accs)}')\n",
    "\n",
    "    return mean_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
