{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# isic_dataloading\n",
    "\n",
    "> How to load isic datasets for supervised learning, and possibly later SSL\n",
    "\n",
    "Important: Note that have to download the data first from https://drive.google.com/drive/folders/1i6fsvK3ASO-AcMHmlQGYWWeM-rK9E_Rz?usp=drive_link. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp isic_dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#directory = \"/content/drive/MyDrive/ISIC_2019_Training_Input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "# from self_supervised.augmentations import *\n",
    "# from self_supervised.layers import *\n",
    "from base_rbt.utils import *\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def unzip_and_cleanup(zip_file_path, target_dir):\n",
    "    \"\"\"\n",
    "    Unzips the dataset files and cleans up the directory structure by removing a hardcoded unwanted prefix.\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_file_path: Full path to the zip file.\n",
    "    - target_dir: Directory where the files will be extracted to and cleaned up.\n",
    "    \"\"\"\n",
    "    # Hardcoded unwanted prefix based on the original zip structure\n",
    "    unwanted_prefix = 'content/drive/MyDrive/ISIC_2019_Zipped'\n",
    "    \n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Unzip the files\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_dir)\n",
    "\n",
    "    cleanup_and_move_files(target_dir, unwanted_prefix)\n",
    "\n",
    "def cleanup_and_move_files(target_dir, unwanted_prefix):\n",
    "    \"\"\"\n",
    "    Moves files from nested directories to the target directory and removes empty directories.\n",
    "    \n",
    "    Parameters:\n",
    "    - target_dir: Target directory where files should ultimately reside.\n",
    "    - unwanted_prefix: Prefix to identify unwanted nested directories for removal.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(target_dir, topdown=False):\n",
    "        # Move files to the target directory and remove empty directories\n",
    "        for name in files:\n",
    "            original_path = os.path.join(root, name)\n",
    "            target_path = os.path.join(target_dir, name)\n",
    "            \n",
    "            if original_path != target_path:\n",
    "                shutil.move(original_path, target_path)\n",
    "        \n",
    "        for name in dirs:\n",
    "            dir_path = os.path.join(root, name)\n",
    "            # Remove the directory if it's part of the unwanted structure or empty\n",
    "            if unwanted_prefix in dir_path or not os.listdir(dir_path):\n",
    "                shutil.rmtree(dir_path, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# # Base directory where the datasets are stored\n",
    "# dataset_dir = \"/content/isic_dataset\"\n",
    "\n",
    "# # Specify the paths for the zip files\n",
    "# training_zip_path = '/content/drive/MyDrive/ISIC_2019_Zipped/ISIC_2019_Training_Resized.zip'\n",
    "# test_zip_path = '/content/drive/MyDrive/ISIC_2019_Zipped/ISIC_2019_Test_Resized.zip'\n",
    "\n",
    "# # Derive the target directories for training and test datasets\n",
    "# training_target_dir = os.path.join(dataset_dir, \"ISIC_2019_Training_Resized\")\n",
    "# test_target_dir = os.path.join(dataset_dir, \"ISIC_2019_Test_Resized\")\n",
    "\n",
    "# # Unzip and cleanup for both datasets\n",
    "# unzip_and_cleanup(training_zip_path, training_target_dir)\n",
    "# unzip_and_cleanup(test_zip_path, test_target_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main dataloader functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_supervised_isic_test_dls(bs, \n",
    "                                 dataset_dir,  # Added parameter for dataset directory\n",
    "                                 size=None,#preset by default to 256.\n",
    "                                 device='cpu', \n",
    "                                 pct_dataset=1.0, \n",
    "                                 num_workers=12):\n",
    "    max_retries = 3\n",
    "    retry_delay = 1\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resized_dir = os.path.join(dataset_dir, \"ISIC_2019_Test_Resized\")\n",
    "            labels_file = os.path.join(resized_dir, \"labels.csv\")\n",
    "\n",
    "            with open(labels_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip the header row\n",
    "                labels = {row[0]: row[1] for row in reader}\n",
    "\n",
    "            resized_fnames = list(labels.keys())\n",
    "\n",
    "            subset_size = int(len(resized_fnames) * pct_dataset)\n",
    "            _resized_fnames = [os.path.join(resized_dir, fname) for fname in resized_fnames[:subset_size]]\n",
    "\n",
    "            dls = ImageDataLoaders.from_path_func(\n",
    "                resized_dir,\n",
    "                _resized_fnames,\n",
    "                lambda x: labels[os.path.basename(x)],\n",
    "                bs=bs,\n",
    "                valid_pct=0,\n",
    "                device=device,\n",
    "                num_workers=num_workers * (device == 'cuda'),\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            return dls\n",
    "        except IOError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed with IOError: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def get_supervised_isic_train_dls(bs, \n",
    "                                  dataset_dir,  # Added parameter for dataset directory\n",
    "                                  size=None, #preset by default to 256.\n",
    "                                  device='cpu',\n",
    "                                  pct_dataset=1.0, \n",
    "                                  num_workers=12):\n",
    "    max_retries = 3\n",
    "    retry_delay = 1\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resized_dir = os.path.join(dataset_dir, \"ISIC_2019_Training_Resized\")\n",
    "            labels_file = os.path.join(resized_dir, \"labels.csv\")\n",
    "\n",
    "            with open(labels_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip the header row\n",
    "                labels = {row[0]: row[1] for row in reader}\n",
    "\n",
    "            resized_fnames = list(labels.keys())\n",
    "\n",
    "            subset_size = int(len(resized_fnames) * pct_dataset)\n",
    "            _resized_fnames = [os.path.join(resized_dir, fname) for fname in resized_fnames[:subset_size]]\n",
    "\n",
    "            dls = ImageDataLoaders.from_path_func(\n",
    "                resized_dir,\n",
    "                _resized_fnames,\n",
    "                lambda x: labels[os.path.basename(x)],\n",
    "                bs=bs,\n",
    "                valid_pct=0,\n",
    "                device=device,\n",
    "                num_workers=num_workers * (device == 'cuda'),\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            return dls\n",
    "        except IOError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed with IOError: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
