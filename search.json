[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "base_rbt",
    "section": "",
    "text": "!pip install git+https://github.com/hamish-haggerty/base_rbt.git#egg='base_rbt'",
    "crumbs": [
      "base_rbt"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "base_rbt",
    "section": "",
    "text": "!pip install git+https://github.com/hamish-haggerty/base_rbt.git#egg='base_rbt'",
    "crumbs": [
      "base_rbt"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "base_rbt",
    "section": "How to use",
    "text": "How to use\nAfter installing, import like this:\n\nfrom base_rbt.all import *\n\nWe also need some other libraries:",
    "crumbs": [
      "base_rbt"
    ]
  },
  {
    "objectID": "base_model.html",
    "href": "base_model.html",
    "title": "base_model",
    "section": "",
    "text": "Here we have the base functions and classes to train a basic BT-style model. Note that this (mostly) all comes directly from here: https://github.com/KeremTurgutlu/self_supervised/blob/main/nbs/14%20-%20barlow_twins.ipynb but we needed to extend some of the functionality for our purposes.\nSome of the base classes and functions needed for image augmentation pipeline:\n\nsource\n\nget_barlow_twins_aug_pipelines\n\n get_barlow_twins_aug_pipelines (size, flip=True, crop=True, noise=True,\n                                 rotate=True, jitter=True, bw=True,\n                                 blur=True, solar=True, cutout=False,\n                                 resize_scale=(0.08, 1.0),\n                                 resize_ratio=(0.75, 1.3333333333333333),\n                                 noise_std=0.025, rotate_deg=30,\n                                 jitter_s=0.6, blur_s=(4, 32),\n                                 blur_r=(0.1, 2), blur_sig=None,\n                                 sol_t=0.05, sol_a=0.05,\n                                 min_dropout_size=(25, 100),\n                                 max_dropout_size=(50, 150), flip_p=0.5,\n                                 rotate_p=0.3, noise_p=0.2, jitter_p=0.3,\n                                 bw_p=0.3, blur_p=0.3, sol_p=0.1,\n                                 cut_p=0.5, same_on_batch=False,\n                                 stats=([0.485, 0.456, 0.406], [0.229,\n                                 0.224, 0.225]), cuda=False, xtra_tfms=[])\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\n\n\n\n\n\nflip\nbool\nTrue\n\n\n\ncrop\nbool\nTrue\n\n\n\nnoise\nbool\nTrue\n\n\n\nrotate\nbool\nTrue\n\n\n\njitter\nbool\nTrue\n\n\n\nbw\nbool\nTrue\n\n\n\nblur\nbool\nTrue\n\n\n\nsolar\nbool\nTrue\n\n\n\ncutout\nbool\nFalse\nWhether to use given aug or not\n\n\nresize_scale\ntuple\n(0.08, 1.0)\n\n\n\nresize_ratio\ntuple\n(0.75, 1.3333333333333333)\n\n\n\nnoise_std\nfloat\n0.025\n\n\n\nrotate_deg\nint\n30\n\n\n\njitter_s\nfloat\n0.6\n\n\n\nblur_s\ntuple\n(4, 32)\nhps of diff augs\n\n\nblur_r\ntuple\n(0.1, 2)\n\n\n\nblur_sig\nNoneType\nNone\n\n\n\nsol_t\nfloat\n0.05\n\n\n\nsol_a\nfloat\n0.05\n\n\n\nmin_dropout_size\ntuple\n(25, 100)\n\n\n\nmax_dropout_size\ntuple\n(50, 150)\nhps of diff augs\n\n\nflip_p\nfloat\n0.5\n\n\n\nrotate_p\nfloat\n0.3\n\n\n\nnoise_p\nfloat\n0.2\n\n\n\njitter_p\nfloat\n0.3\n\n\n\nbw_p\nfloat\n0.3\n\n\n\nblur_p\nfloat\n0.3\n\n\n\nsol_p\nfloat\n0.1\n\n\n\ncut_p\nfloat\n0.5\nprob of performing aug\n\n\nsame_on_batch\nbool\nFalse\n\n\n\nstats\ntuple\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\ncuda\nbool\nFalse\n\n\n\nxtra_tfms\nlist\n[]\n\n\n\n\n\nsource\n\n\nget_multi_aug_pipelines\n\n get_multi_aug_pipelines (size, flip=True, crop=True, noise=True,\n                          rotate=True, jitter=True, bw=True, blur=True,\n                          solar=True, cutout=False, resize_scale=(0.08,\n                          1.0), resize_ratio=(0.75, 1.3333333333333333),\n                          noise_std=0.025, rotate_deg=30, jitter_s=0.6,\n                          blur_s=(4, 32), blur_r=(0.1, 2), blur_sig=None,\n                          sol_t=0.05, sol_a=0.05, min_dropout_size=(25,\n                          100), max_dropout_size=(50, 150), flip_p=0.5,\n                          rotate_p=0.3, noise_p=0.2, jitter_p=0.3,\n                          bw_p=0.3, blur_p=0.3, sol_p=0.1, cut_p=0.5,\n                          same_on_batch=False, stats=([0.485, 0.456,\n                          0.406], [0.229, 0.224, 0.225]), cuda=False,\n                          xtra_tfms=[])\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\n\n\n\n\n\nflip\nbool\nTrue\n\n\n\ncrop\nbool\nTrue\n\n\n\nnoise\nbool\nTrue\n\n\n\nrotate\nbool\nTrue\n\n\n\njitter\nbool\nTrue\n\n\n\nbw\nbool\nTrue\n\n\n\nblur\nbool\nTrue\n\n\n\nsolar\nbool\nTrue\n\n\n\ncutout\nbool\nFalse\nWhether to use given aug or not\n\n\nresize_scale\ntuple\n(0.08, 1.0)\n\n\n\nresize_ratio\ntuple\n(0.75, 1.3333333333333333)\n\n\n\nnoise_std\nfloat\n0.025\n\n\n\nrotate_deg\nint\n30\n\n\n\njitter_s\nfloat\n0.6\n\n\n\nblur_s\ntuple\n(4, 32)\nhps of diff augs\n\n\nblur_r\ntuple\n(0.1, 2)\n\n\n\nblur_sig\nNoneType\nNone\n\n\n\nsol_t\nfloat\n0.05\n\n\n\nsol_a\nfloat\n0.05\n\n\n\nmin_dropout_size\ntuple\n(25, 100)\n\n\n\nmax_dropout_size\ntuple\n(50, 150)\nhps of diff augs\n\n\nflip_p\nfloat\n0.5\n\n\n\nrotate_p\nfloat\n0.3\n\n\n\nnoise_p\nfloat\n0.2\n\n\n\njitter_p\nfloat\n0.3\n\n\n\nbw_p\nfloat\n0.3\n\n\n\nblur_p\nfloat\n0.3\n\n\n\nsol_p\nfloat\n0.1\n\n\n\ncut_p\nfloat\n0.5\nprob of performing aug\n\n\nsame_on_batch\nbool\nFalse\n\n\n\nstats\ntuple\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\ncuda\nbool\nFalse\n\n\n\nxtra_tfms\nlist\n[]\n\n\n\n\n\nsource\n\n\nget_BT_batch_augs\n\n get_BT_batch_augs (size, flip=True, crop=True, noise=True, rotate=True,\n                    jitter=True, bw=True, blur=True, solar=True,\n                    cutout=False, resize_scale=(0.08, 1.0),\n                    resize_ratio=(0.75, 1.3333333333333333),\n                    noise_std=0.025, rotate_deg=30, jitter_s=0.6,\n                    blur_s=(4, 32), blur_r=(0.1, 2), blur_sig=None,\n                    sol_t=0.05, sol_a=0.05, min_dropout_size=(25, 100),\n                    max_dropout_size=(50, 150), flip_p=0.5, rotate_p=0.3,\n                    noise_p=0.2, jitter_p=0.3, bw_p=0.3, blur_p=0.3,\n                    sol_p=0.1, cut_p=0.5, same_on_batch=False,\n                    stats=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    cuda=False, xtra_tfms=[])\n\nInput batch augmentations implemented in tv+kornia+fastai\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\n\n\n\n\n\nflip\nbool\nTrue\n\n\n\ncrop\nbool\nTrue\n\n\n\nnoise\nbool\nTrue\n\n\n\nrotate\nbool\nTrue\n\n\n\njitter\nbool\nTrue\n\n\n\nbw\nbool\nTrue\n\n\n\nblur\nbool\nTrue\n\n\n\nsolar\nbool\nTrue\n\n\n\ncutout\nbool\nFalse\nWhether to use given aug or not\n\n\nresize_scale\ntuple\n(0.08, 1.0)\n\n\n\nresize_ratio\ntuple\n(0.75, 1.3333333333333333)\n\n\n\nnoise_std\nfloat\n0.025\n\n\n\nrotate_deg\nint\n30\n\n\n\njitter_s\nfloat\n0.6\n\n\n\nblur_s\ntuple\n(4, 32)\nhps of diff augs\n\n\nblur_r\ntuple\n(0.1, 2)\n\n\n\nblur_sig\nNoneType\nNone\n\n\n\nsol_t\nfloat\n0.05\n\n\n\nsol_a\nfloat\n0.05\n\n\n\nmin_dropout_size\ntuple\n(25, 100)\n\n\n\nmax_dropout_size\ntuple\n(50, 150)\nhps of diff augs\n\n\nflip_p\nfloat\n0.5\n\n\n\nrotate_p\nfloat\n0.3\n\n\n\nnoise_p\nfloat\n0.2\n\n\n\njitter_p\nfloat\n0.3\n\n\n\nbw_p\nfloat\n0.3\n\n\n\nblur_p\nfloat\n0.3\n\n\n\nsol_p\nfloat\n0.1\n\n\n\ncut_p\nfloat\n0.5\nprob of performing aug\n\n\nsame_on_batch\nbool\nFalse\n\n\n\nstats\ntuple\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\ncuda\nbool\nFalse\n\n\n\nxtra_tfms\nlist\n[]\n\n\n\n\n\nsource\n\n\nRandomCenterDropout\n\n RandomCenterDropout (p=0.5, min_dropout_size=(20, 20),\n                      max_dropout_size=(60, 60), fill_value=0,\n                      same_on_batch=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nRandomGaussianBlur\n\n RandomGaussianBlur (p=1.0, prob=0.5, s=(8, 32), sig=None, blur_r=(0.1,\n                     2), same_on_batch=False, **kwargs)\n\nRandomly apply gaussian blur with probability p with a value of s\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n1.0\ndebugging (bug in libraries implementation)\n\n\nprob\nfloat\n0.5\nthe real probability\n\n\ns\ntuple\n(8, 32)\nkernel\n\n\nsig\nNoneType\nNone\nsig_val is either manually input OR\n\n\nblur_r\ntuple\n(0.1, 2)\nis randomly chosen from uniform with these bounds\n\n\nsame_on_batch\nbool\nFalse\n\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nget_bt_aug_pipelines\n\n get_bt_aug_pipelines (bt_augs, size)\n\n\nsource\n\n\nget_bt_dermnet_aug_pipelines\n\n get_bt_dermnet_aug_pipelines (size)\n\n\nsource\n\n\nget_bt_imagenet_aug_pipelines\n\n get_bt_imagenet_aug_pipelines (size)\n\n\nsource\n\n\nhelper_get_bt_augs\n\n helper_get_bt_augs (size, Augs={'flip_p1': 0.5, 'flip_p2': 0.5,\n                     'jitter_p1': 0.8, 'jitter_p2': 0.8, 'bw_p1': 0.2,\n                     'bw_p2': 0.2, 'blur_p1': 1.0, 'blur_p2': 0.1,\n                     'sol_p1': 0.0, 'sol_p2': 0.2, 'noise_p1': 0.0,\n                     'noise_p2': 0.0, 'cut_p': 0.5, 'resize_scale': (0.7,\n                     1.0), 'resize_ratio': (0.75, 1.3333333333333333),\n                     'rotate_deg': 45.0, 'rotate_p': 0.5, 'blur_r': (0.1,\n                     2), 'blur_s': 13, 'sol_t': 0.1, 'sol_a': 0.1,\n                     'noise_std': 0.1, 'min_dropout_size': (25, 100),\n                     'max_dropout_size': (50, 150)})\n\n\nsource\n\n\nget_bt_cifar10_aug_pipelines\n\n get_bt_cifar10_aug_pipelines (size)\n\n\nsource\n\n\nget_ssl_dls\n\n get_ssl_dls (dataset, bs, size, device, pct_dataset=1.0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\n\n\ncifar10, dermnet, etc\n\n\nbs\n\n\n\n\n\nsize\n\n\n\n\n\ndevice\n\n\n\n\n\npct_dataset\nfloat\n1.0\n\n\n\n\nBase functions / classes we need to train a BT / RBT model.\n\nsource\n\n\nBarlowTwins\n\n BarlowTwins (aug_pipelines, n_in, lmb, sparsity_level,\n              model_type='barlow_twins', print_augs=False)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\ncreate_barlow_twins_model\n\n create_barlow_twins_model (encoder, hidden_size=256, projection_size=128,\n                            bn=True, nlayers=3)\n\nCreate Barlow Twins model\n\nsource\n\n\nBarlowTwinsModel\n\n BarlowTwinsModel (encoder, projector)\n\nAn encoder followed by a projector\n\nsource\n\n\ncreate_barlow_twins_model\n\n create_barlow_twins_model (encoder, hidden_size=256, projection_size=128,\n                            bn=True, nlayers=3)\n\nCreate Barlow Twins model\n\nsource\n\n\nBarlowTwinsModel\n\n BarlowTwinsModel (encoder, projector)\n\nAn encoder followed by a projector\nHere we write down standard definition of lf for RAT method:\nBarlowTwins needs an lf method to work properly. Here we provide the lf of standard barlow twins. Later we can patch in a new defintion of lf that involves random functions, inner maximization etc. The tools needed to do this are provised in base_lf\n\nsource\n\n\nlf_bt\n\n lf_bt (pred, I, lmb)\n\n\nsource\n\n\nlf_bt_indiv_sparse\n\n lf_bt_indiv_sparse (pred, I, lmb, sparsity_level)\n\n\nsource\n\n\nlf_bt_group_sparse\n\n lf_bt_group_sparse (pred, I, lmb, sparsity_level)\n\n\nsource\n\n\nlf_bt_group_norm_sparse\n\n lf_bt_group_norm_sparse (pred, I, lmb, sparsity_level)\n\n\nsource\n\n\nlf_bt_fun\n\n lf_bt_fun (pred, I, lmb, sparsity_level)\n\n\nsource\n\n\nlf_bt_proj_group_sparse\n\n lf_bt_proj_group_sparse (pred, I, lmb, sparsity_level)\n\n\nsource\n\n\nBarlowTwins.lf\n\n BarlowTwins.lf (pred, *yb)\n\nAssumes model created according to type p3\n\nsource\n\n\nmy_splitter_bt\n\n my_splitter_bt (m)\n\nHere we show how to use the above functions in an end to end fashion. First we get some data and plonk it into a dls, Then create an encoder, an augmentation pipeline, a learner, then fit the learner. This is the complete process of training BT.\n\nsource\n\n\nshow_bt_batch\n\n show_bt_batch (dls, n_in, aug, n=2, print_augs=True)\n\nGiven a linear learner, show a batch\n\nsource\n\n\nBarlowTrainer\n\n BarlowTrainer (model, dls, bt_aug_pipelines, lmb, sparsity_level, n_in,\n                model_type, wd, device, num_it=100,\n                load_learner_path=None, experiment_dir=None,\n                start_epoch=0, save_interval=None)\n\nSetup a learner for training a BT model. Can do transfer learning, normal training, or resume training.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nAn encoder followed by a projector\n\n\ndls\n\n\n\n\n\nbt_aug_pipelines\n\n\n\n\n\nlmb\n\n\n\n\n\nsparsity_level\n\n\n\n\n\nn_in\n\n\n\n\n\nmodel_type\n\n\n\n\n\nwd\n\n\n\n\n\ndevice\n\n\n\n\n\nnum_it\nint\n100\nNumber of iterations to run lr_find for.\n\n\nload_learner_path\nNoneType\nNone\nPath to load learner from (optional)\n\n\nexperiment_dir\nNoneType\nNone\nWhere to save model checkpoints (optional)\n\n\nstart_epoch\nint\n0\nWhich epoch to start from\n\n\nsave_interval\nNoneType\nNone\nHow often to save model checkpoints (optional).\n\n\n\n\nsource\n\n\nmain_bt_train\n\n main_bt_train (config, start_epoch=0, interrupt_epoch=100,\n                load_learner_path=None, learn_type='standard',\n                experiment_dir=None)\n\nBasically map from config to training a BT model. Optionally save checkpoints of learner.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\n\n\n\nstart_epoch\nint\n0\n\n\n\ninterrupt_epoch\nint\n100\n\n\n\nload_learner_path\nNoneType\nNone\n\n\n\nlearn_type\nstr\nstandard\ncan be ‘standard’, ‘transfer_learning’, or ‘continue_learning’\n\n\nexperiment_dir\nNoneType\nNone\n\n\n\n\n\n# config_path = '../configs/cifar10/bt_test_config.yaml'\n# config = load_config(config_path)\n\n# load_learner_path=None\n# experiment_dir=None\n# start_epoch=0\n\n\n# # Initialize the device for model training (CUDA or CPU)\n# device = default_device()\n\n# # Construct the model based on the configuration\n# # This involves selecting the architecture and setting model-specific hyperparameters.\n# encoder = resnet_arch_to_encoder(arch=config.arch, weight_type=config.weight_type)\n\n# model = create_barlow_twins_model(encoder, hidden_size=config.hs, projection_size=config.ps)\n\n# # Prepare data loaders according to the dataset specified in the configuration\n# dls = get_ssl_dls(dataset=config.dataset, bs=config.bs,size=config.size, device=device,pct_dataset=config.pct_dataset)\n\n# # Set up data augmentation pipelines as specified in the configuration\n# bt_aug_pipelines = get_bt_aug_pipelines(bt_augs=config.bt_augs, size=config.size)\n\n# # Train the model with the specified configurations and save `learn` checkpoints\n\n# #Setup the bt trainer. basically a `Learner` with a few extra bells and whistles\n# bt_trainer = BarlowTrainer(model=model,\n#                dls=dls,\n#                bt_aug_pipelines=bt_aug_pipelines,\n#                lmb=config.lmb,\n#                sparsity_level=config.sparsity_level,\n#                n_in=config.n_in,\n#                model_type=config.model_type,\n#                wd=config.wd,\n#                num_it=config.num_it,\n#                device=device,\n#                load_learner_path=load_learner_path,\n#                experiment_dir=experiment_dir,\n#                start_epoch=start_epoch,\n#                save_interval=config.save_interval\n#                               )\n\n\n# bt_trainer.learn.lr_find??\n\n\nsource\n\n\nget_bt_experiment_state\n\n get_bt_experiment_state (config, base_dir)\n\nGet the load_learner_path, learn_type, start_epoch, interrupt_epoch for BT experiment. Basically this tells us how to continue learning (e.g. we have run two sessions for 100 epochs, and want to continue for another 100 epochs). Return values are None if we are starting from scratch.\n\nsource\n\n\nmain_bt_experiment\n\n main_bt_experiment (config, base_dir)\n\nRun several epochs of the experiment as defined in the config and where we are up to. e.g. epoch 0, or resuming at epoch 99 etc. Basically a stateful version of main_bt_train that can be resumed. And saving.\nFull example.\nExample where the experiment_index.json already exists in base_dir (project root).",
    "crumbs": [
      "base_model"
    ]
  },
  {
    "objectID": "base_supervised.html",
    "href": "base_supervised.html",
    "title": "base_supervised",
    "section": "",
    "text": "API:\n\nTrain and then test linear head. Requires inputs: encoder, dls_val, augpipe_val, indim,outdim, num_epochs,\n\n\nsource\n\nget_linear_batch_augs\n\n get_linear_batch_augs (size, resize=True, resize_scale=(0.08, 1.0),\n                        resize_ratio=(0.75, 1.3333333333333333),\n                        stats=None, cuda=False, xtra_tfms=[])\n\nInput batch augmentations implemented in tv+kornia+fastai\nThe model for linear evaluation and semi-supervised learning requires an encoder and a (randomly) initialised head.\n\nsource\n\n\nLM\n\n LM (encoder, numout, encoder_dimension=2048)\n\nBasic linear model\nThe ‘callback’ for linear evaluation is the following:\n\nsource\n\n\nLinearBt\n\n LinearBt (aug_pipelines, n_in, show_batch=False, print_augs=False,\n           data=None, tune_model_path=None, tune_save_after=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nTest:\nExample usage: First inputs needed. In the next cell we get dls_val and dls_test.\nAugmentations and learner:\n\nsource\n\n\nshow_linear_batch\n\n show_linear_batch (dls, n_in, aug, n=2, print_augs=True)\n\nGiven a linear learner, show a batch\n\n# def get_bt_cifar10_aug_pipelines(size):\n#     aug_pipelines_1 = get_barlow_twins_aug_pipelines(size=size,\n#                                                     bw=True, rotate=True,noise=True, jitter=True, blur=True,solar=True,\n#                                                     resize_scale=(0.4, 1.0),rotate_deg=45,noise_std=0.0125, jitter_s=1.0, blur_s=math.ceil(size/10)+1,\n#                                                     bw_p=0.2, flip_p=0.5,rotate_p=0.25,noise_p=0.5, jitter_p=0.5, blur_p=0.5,sol_p=0.0,\n#                                                     stats=cifar_stats,same_on_batch=False, xtra_tfms=[]\n#                                                     )\n\n#     aug_pipelines_2 = get_barlow_twins_aug_pipelines(size=size,\n#                                                     bw=True, rotate=True,noise=True, jitter=True, blur=True,solar=True,\n#                                                     resize_scale=(0.4, 1.0),rotate_deg=45,noise_std=0.0125, jitter_s=1.0, blur_s=math.ceil(size/10)+1,sol_t=0.01,sol_a=0.01,\n#                                                     bw_p=0.2, flip_p=0.5,rotate_p=0.25,noise_p=0.5, jitter_p=0.5, blur_p=0.1,sol_p=0.2,\n#                                                     stats=cifar_stats,same_on_batch=False, xtra_tfms=[]\n#                                                     )\n\n#     bt_cifar10_aug_pipelines = [aug_pipelines_1,aug_pipelines_2]\n\n#     return bt_cifar10_aug_pipelines\n\n# #Add other augmentations here e.g. BYOL augs\n\n# bt_aug_func_dict = {'bt_cifar10_aug_pipelines':get_bt_cifar10_aug_pipelines}\n\n\n# def get_bt_aug_pipelines(bt_augs,size):\n\n#     return bt_aug_func_dict[bt_augs](size)\n\n\nsource\n\n\nget_supervised_dls\n\n get_supervised_dls (dataset, pct_dataset_train, pct_dataset_test, bs,\n                     bs_test, size, device)\n\nGet train and test dataloaders for supervised learning\n\nsource\n\n\nget_supervised_aug_pipelines\n\n get_supervised_aug_pipelines (augs, size)\n\n\nsource\n\n\nget_supervised_isic_augmentations\n\n get_supervised_isic_augmentations (size)\n\n\nsource\n\n\nget_supervised_cifar10_augmentations\n\n get_supervised_cifar10_augmentations (size)\n\nTest:\nThis enables us to freeze the encoder as needed:\n\nsource\n\n\nencoder_head_splitter\n\n encoder_head_splitter (m)\n\nSupervisedLearning allows us to perform either linear evaluation, semi-supervised learning, or standard supervised learning.\n\nsource\n\n\nSaveLearnerAfterFit\n\n SaveLearnerAfterFit (experiment_dir, num_run=0, with_opt=True)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nSupervisedLearning\n\n SupervisedLearning (model, dls_train, aug_pipelines_supervised, n_in, wd,\n                     device, num_it=100, num_run=None,\n                     experiment_dir=None)\n\nTrain model using supervised learning. Either linear evaluation or semi-supervised.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\n\n\n\ndls_train\n\n\n\n\n\naug_pipelines_supervised\n\n\n\n\n\nn_in\n\n\n\n\n\nwd\n\n\n\n\n\ndevice\n\n\n\n\n\nnum_it\nint\n100\n\n\n\nnum_run\nNoneType\nNone\nn of num_runs. e.g. num_runs=5 and num_run=3 means this is the 3rd run.\n\n\nexperiment_dir\nNoneType\nNone\nBasically just tells what name to save checkpoint as - if applicable.\n\n\n\n\nsource\n\n\nget_encoder\n\n get_encoder (arch, weight_type, load_learner_path=None, device='cpu')\n\nGet an encoder for supervised learner. If load_learner_path is not None, then load the learner and return the encoder.\n\nsource\n\n\nmain_sup_train\n\n main_sup_train (config, load_learner_path=None, num_run=None,\n                 experiment_dir=None)\n\nBasically map from config to training a supervised model. Optionally save checkpoints of learner.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\n\n\n\nload_learner_path\nNoneType\nNone\npath to load encoder if applicable\n\n\nnum_run\nNoneType\nNone\nrun we are up to - tell us what name to give the saved checkpoint, if applicable.\n\n\nexperiment_dir\nNoneType\nNone\nwhere to save checkpoints\n\n\n\n\nsource\n\n\nget_supervised_experiment_state\n\n get_supervised_experiment_state (config, base_dir)\n\nGet the load_learner_path, num_run, for supervised experiment. Basically tells us what run we are up to. load_learner_path is the path to the highest numbered checkpoint. so far. num_run is the number of the next run. If num_run&gt;config.num_runs, then we are done.\n\nsource\n\n\nmain_sup_experiment\n\n main_sup_experiment (config, base_dir, load_learner_path=None)\n\nRun a supervised learning experiment with the given configuration and save the results to the experiment directory. Return the experiment directory and experiment hash.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\n\n\n\nbase_dir\n\n\n\n\n\nload_learner_path\nNoneType\nNone\npath to load encoder if applicable\n\n\n\nFull example",
    "crumbs": [
      "base_supervised"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\ntest_grad_off\n\n test_grad_off (model)\n\nTest that all non-batch norm grads are off, but batch norm grads are on.\n\nsource\n\n\ntest_grad_on\n\n test_grad_on (model)\n\nTest that all grads are on for modules with parameters.\n\nsource\n\n\nseed_everything\n\n seed_everything (seed=42)\n\n” Seed everything.\n\nsource\n\n\nload_config\n\n load_config (file_path)\n\n\nsource\n\n\nadjust_config_with_derived_values\n\n adjust_config_with_derived_values (config)\n\nBasic network mostly used for testing.\n\nsource\n\n\nresnet_arch_to_encoder\n\n resnet_arch_to_encoder\n                         (arch:Literal['smallres','resnet18','resnet34','r\n                         esnet50'], weight_type:Literal['random','imgnet_b\n                         t_pretrained','imgnet_sup_pretrained']='random')\n\nGiven a ResNet architecture, return the encoder configured for 3 input channels. The ‘weight_type’ argument specifies the weight initialization strategy.\nArgs: arch (Literal[‘smallres’,‘resnet18’, ‘resnet34’, ‘resnet50’]): The architecture of the ResNet. weight_type (Literal[‘random’, ‘imgnet_bt_pretrained’, ‘imgnet_sup_pretrained’]): Specifies the weight initialization strategy. Defaults to ‘random’.\nReturns: Encoder: An encoder configured for 3 input channels and specified architecture.\n\nsource\n\n\nget_resnet_encoder\n\n get_resnet_encoder (model, n_in=3)\n\n\nsource\n\n\ngenerate_config_hash\n\n generate_config_hash (config)\n\nGenerates a unique hash for a given experiment configuration.\nArgs: config (dict or Namespace): Experiment configuration. Can be a dictionary or a namespace object.\nReturns: str: A unique hash representing the experiment configuration.\nTest generate_config_hash\n\nconfig1 = SimpleNamespace(arch='resnet18', dataset='cifar10', n_in=3, encoder_dimension=512)\nconfig2 = SimpleNamespace(arch='resnet34', dataset='cifar10', n_in=3, encoder_dimension=512)\nconfig3 = SimpleNamespace(arch='resnet50', dataset='cifar10', n_in=3, encoder_dimension=2048)\n\ntest_eq(generate_config_hash(config1), generate_config_hash(config1))\ntest_ne(generate_config_hash(config1), generate_config_hash(config2))\ntest_ne(generate_config_hash(config1), generate_config_hash(config3))\ntest_ne(generate_config_hash(config2), generate_config_hash(config3))\n\nconfig4 = SimpleNamespace(dataset='cifar10', arch='resnet18', n_in=3, encoder_dimension=512)  # Different order\ntest_eq(generate_config_hash(config1), generate_config_hash(config4))\n\n\nsource\n\n\nsetup_experiment\n\n setup_experiment (config, base_dir)\n\n\nsource\n\n\nget_latest_commit_hash\n\n get_latest_commit_hash (repo_path)\n\n\nsource\n\n\nupdate_experiment_index\n\n update_experiment_index (project_root, details)\n\n\nsource\n\n\nsave_metadata_file\n\n save_metadata_file (experiment_dir, git_commit_hash)\n\nSaves a metadata file with the Git commit hash\n\nsource\n\n\nsave_configuration\n\n save_configuration (config, experiment_dir)\n\nSaves the experiment configuration as a YAML file in the experiment directory.\nArgs: config (dict, Namespace, or any serializable object): Experiment configuration. experiment_dir (str): Path to the directory where the config file will be saved.\n\nsource\n\n\ncreate_experiment_directory\n\n create_experiment_directory (base_dir, config)\n\n\nsource\n\n\nSaveLearnerCheckpoint\n\n SaveLearnerCheckpoint (experiment_dir, start_epoch=0, save_interval=250,\n                        with_opt=True)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nInterruptCallback\n\n InterruptCallback (interrupt_epoch)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nget_highest_num_path\n\n get_highest_num_path (base_dir, config)\n\nCheck in all experiment directories derived from the config and return the path to the file with the highest number along with its experiment directory.\n\nsource\n\n\nreturn_max_filename\n\n return_max_filename (filename1, filename2)\n\n\nsource\n\n\nfind_largest_file\n\n find_largest_file (directory_path)\n\nFind the file with the largest number (e.g. epoch) in a directory.\n\nsource\n\n\nextract_number\n\n extract_number (filename)\n\nExtract the number from end of filename. e.g. epoch\n\nsource\n\n\nload_dict_from_gdrive\n\n load_dict_from_gdrive (directory, filename)\n\n\nsource\n\n\nsave_dict_to_gdrive\n\n save_dict_to_gdrive (d, directory, filename)\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "metrics",
    "section": "",
    "text": "source\n\n\n\n predict_model (xval, yval, model, aug_pipelines_test, numavg=3,\n                criterion=FlattenedLoss of CrossEntropyLoss(),\n                deterministic=False)\n\nNote that this assumes xval is entire validation set. If it doesn’t fit in memory, can’t use this guy\n\nsource\n\n\n\n\n predict_ensemble (yval, scores1, scores2)\n\nscores can be normalized (softmax) or not\n\nsource\n\n\n\n\n classification_report_wrapper (ypred, y, int_to_classes,\n                                print_report=True)\n\n\nsource\n\n\n\n\n format_classification_report (data_dict)\n\n\nsource\n\n\n\n\n Mean_Report (reports, classes)\n\n\nsource\n\n\n\n\n Mean_Results (results, vocab)\n\nGet mean classif report and display it\n\nsource\n\n\n\n\n print_confusion_matrix (ypred, y, vocab)\n\nWe monkey patch some plotting functions from scikitplot and edit them - we need greater control of legend etc\n\nsource\n\n\n\n\n plot_pr (ytest, probs, int_to_classes)\n\n\nsource\n\n\n\n\n plot_roc (ytest, probs, int_to_classes)\n\nFunctions to plot ROC curve and precision-recall curves:\n\nsource\n\n\n\n\n plot_pr (ytest, probs, int_to_classes)\n\n\nsource\n\n\n\n\n plot_roc (ytest, probs, int_to_classes)\n\n\nsource\n\n\n\n\n Pr_Dict (ytest, probs, int_to_classes=None)\n\nMostly used to verify results of plot (debug)\n\nsource\n\n\n\n\n Auc_Dict (ytest, probs, int_to_classes=None)\n\nMostly used to verify results of plot (debug)\n\nsource\n\n\n\n\n Mean_Results (results, vocab)\n\nGet mean classif report and display it\n\nsource\n\n\n\n\n get_xval_metrics (xval, yval, model, aug_pipelines_test, int_to_classes,\n                   numavg=3)\n\nget metrics from gives batch (xval,yval)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nxval\n\n\n\n\n\nyval\n\n\n\n\n\nmodel\n\n\n\n\n\naug_pipelines_test\n\n\n\n\n\nint_to_classes\n\n\n\n\n\nnumavg\nint\n3\nnote that we can’t call dls.vocab as it might be smaller on the test set\n\n\n\n\nsource\n\n\n\n\n get_dls_metrics (dls, model, aug_pipelines_test, int_to_classes)\n\nget metrics from model and dataloader\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ndls\n\n\n\nmodel\n\n\n\naug_pipelines_test\n\n\n\nint_to_classes\nnote that we can’t call dls.vocab as it might be smaller on the test set\n\n\n\n\nsource\n\n\n\n\n predict_whole_model (dls_test, model, aug_pipelines_test, numavg=3,\n                      criterion=FlattenedLoss of CrossEntropyLoss(),\n                      deterministic=False)\n\nPredicts the labels and probabilities for the entire test set using the specified model and data augmentation pipelines. Returns a dictionary containing the labels, probabilities, predicted labels, and accuracy.\nArgs: dls_test: The test dataloader. model: The trained model. aug_pipelines_test: The test data augmentation pipelines. numavg: The number of times to perform test-time augmentation. criterion: The loss function to use for computing the accuracy. deterministic: Whether to use deterministic computation.\nReturns: A dictionary containing the labels, probabilities, predicted labels, and accuracy.",
    "crumbs": [
      "metrics"
    ]
  },
  {
    "objectID": "metrics.html#predictions-given-xval-and-yval",
    "href": "metrics.html#predictions-given-xval-and-yval",
    "title": "metrics",
    "section": "",
    "text": "source\n\n\n\n predict_model (xval, yval, model, aug_pipelines_test, numavg=3,\n                criterion=FlattenedLoss of CrossEntropyLoss(),\n                deterministic=False)\n\nNote that this assumes xval is entire validation set. If it doesn’t fit in memory, can’t use this guy\n\nsource\n\n\n\n\n predict_ensemble (yval, scores1, scores2)\n\nscores can be normalized (softmax) or not\n\nsource\n\n\n\n\n classification_report_wrapper (ypred, y, int_to_classes,\n                                print_report=True)\n\n\nsource\n\n\n\n\n format_classification_report (data_dict)\n\n\nsource\n\n\n\n\n Mean_Report (reports, classes)\n\n\nsource\n\n\n\n\n Mean_Results (results, vocab)\n\nGet mean classif report and display it\n\nsource\n\n\n\n\n print_confusion_matrix (ypred, y, vocab)\n\nWe monkey patch some plotting functions from scikitplot and edit them - we need greater control of legend etc\n\nsource\n\n\n\n\n plot_pr (ytest, probs, int_to_classes)\n\n\nsource\n\n\n\n\n plot_roc (ytest, probs, int_to_classes)\n\nFunctions to plot ROC curve and precision-recall curves:\n\nsource\n\n\n\n\n plot_pr (ytest, probs, int_to_classes)\n\n\nsource\n\n\n\n\n plot_roc (ytest, probs, int_to_classes)\n\n\nsource\n\n\n\n\n Pr_Dict (ytest, probs, int_to_classes=None)\n\nMostly used to verify results of plot (debug)\n\nsource\n\n\n\n\n Auc_Dict (ytest, probs, int_to_classes=None)\n\nMostly used to verify results of plot (debug)\n\nsource\n\n\n\n\n Mean_Results (results, vocab)\n\nGet mean classif report and display it\n\nsource\n\n\n\n\n get_xval_metrics (xval, yval, model, aug_pipelines_test, int_to_classes,\n                   numavg=3)\n\nget metrics from gives batch (xval,yval)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nxval\n\n\n\n\n\nyval\n\n\n\n\n\nmodel\n\n\n\n\n\naug_pipelines_test\n\n\n\n\n\nint_to_classes\n\n\n\n\n\nnumavg\nint\n3\nnote that we can’t call dls.vocab as it might be smaller on the test set\n\n\n\n\nsource\n\n\n\n\n get_dls_metrics (dls, model, aug_pipelines_test, int_to_classes)\n\nget metrics from model and dataloader\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ndls\n\n\n\nmodel\n\n\n\naug_pipelines_test\n\n\n\nint_to_classes\nnote that we can’t call dls.vocab as it might be smaller on the test set\n\n\n\n\nsource\n\n\n\n\n predict_whole_model (dls_test, model, aug_pipelines_test, numavg=3,\n                      criterion=FlattenedLoss of CrossEntropyLoss(),\n                      deterministic=False)\n\nPredicts the labels and probabilities for the entire test set using the specified model and data augmentation pipelines. Returns a dictionary containing the labels, probabilities, predicted labels, and accuracy.\nArgs: dls_test: The test dataloader. model: The trained model. aug_pipelines_test: The test data augmentation pipelines. numavg: The number of times to perform test-time augmentation. criterion: The loss function to use for computing the accuracy. deterministic: Whether to use deterministic computation.\nReturns: A dictionary containing the labels, probabilities, predicted labels, and accuracy.",
    "crumbs": [
      "metrics"
    ]
  },
  {
    "objectID": "cifar10_dataloading.html",
    "href": "cifar10_dataloading.html",
    "title": "cifar10_dataloading",
    "section": "",
    "text": "CIFAR10\n\nsource\n\nlabel_func\n\n label_func (fname)\n\n\nsource\n\n\nload_cifar10_test_data\n\n load_cifar10_test_data (pct_dataset=1.0)\n\n\nsource\n\n\nload_cifar10_train_data\n\n load_cifar10_train_data (pct_dataset=1.0)\n\n\nsource\n\n\nget_supervised_cifar10_test_dls\n\n get_supervised_cifar10_test_dls (bs, size, device, pct_dataset=1.0,\n                                  num_workers=12)\n\n\nsource\n\n\nget_supervised_cifar10_train_dls\n\n get_supervised_cifar10_train_dls (bs, size, device, pct_dataset=1.0,\n                                   num_workers=12)\n\n\nsource\n\n\nget_bt_cifar10_train_dls\n\n get_bt_cifar10_train_dls (bs, size, device, pct_dataset=1.0,\n                           num_workers=12)",
    "crumbs": [
      "cifar10_dataloading"
    ]
  },
  {
    "objectID": "dermnet_dataloading.html",
    "href": "dermnet_dataloading.html",
    "title": "derment_dataloading",
    "section": "",
    "text": "source\n\nget_bt_dermnet_train_dls\n\n get_bt_dermnet_train_dls (bs, size, device, pct_dataset=1.0,\n                           num_workers=12)\n\n\nsource\n\n\nlabel_func\n\n label_func (x)",
    "crumbs": [
      "derment_dataloading"
    ]
  },
  {
    "objectID": "base_lf.html",
    "href": "base_lf.html",
    "title": "base_lf",
    "section": "",
    "text": "Here we have the base functions and classes to build lf variations\n\nsource\n\nseed_everything\n\n seed_everything (seed=42)\n\n” Seed everything.\n\nsource\n\n\nrandom_sinusoid\n\n random_sinusoid (x, std=0.1, seed=0)\n\nTest that shape of output is as expected:\nNote: There is something funny going on with random seed. For now, I’m going to just treat the sinusoids as completely random and see how we go. Since the batch size will be smaller might not be an issue\n\nsource\n\n\nC_z1z2\n\n C_z1z2 (z1norm, z1norm_2, z2norm, z2norm_2, indep=True)\n\nTest that shape of output is as expected:\n\nbs,d = 32,100 \nz1norm,z1norm_2,z2norm,z2norm_2 = torch.rand(32,100),torch.rand(32,100),torch.rand(32,100),torch.rand(32,100)\n\ntest(C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2).shape,torch.rand(d,d).shape,  all_equal)\n\n\nsource\n\n\nMax_Corr\n\n Max_Corr (qs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\n\n\n\nDetails\n\n\n\n\nqs\nqs will tend to be =ps i.e. projection dimension, although this is not required.\n\n\n\nTest that shape of output is as expected:\n\nsource\n\n\nCdiff_Sup\n\n Cdiff_Sup (I, qs, inner_steps, normal, std, s_unif, default, indep=True)\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "base_lf"
    ]
  },
  {
    "objectID": "isic_dataloading.html",
    "href": "isic_dataloading.html",
    "title": "isic_dataloading",
    "section": "",
    "text": "Following is a bunch of helper functions to construct the fnames and other things used to construct the dls builder functions.\nAdmiteddly this following cell is a bit opaque, but the functions here are only ever used once.\n\nsource\n\nis_colab\n\n is_colab ()\n\n\nsource\n\n\nload_data\n\n load_data (load=False)\n\n\nsource\n\n\nget_pct_dataset\n\n get_pct_dataset (fnames, labels, pct_dataset=1.0)\n\n\nsource\n\n\nget_data\n\n get_data (load=False)\n\n\nsource\n\n\nget_fnames\n\n get_fnames (_fnames, _labels, label_func)\n\n\nsource\n\n\nget_difference\n\n get_difference (x1, x2)\n\n\nsource\n\n\nget_label_func_dict\n\n get_label_func_dict (_fnames)\n\n\nsource\n\n\nget_class_from_id\n\n get_class_from_id (string)\n\nGiven the identifier e.g. ISIC_0000000.jpg return the class label\n\nsource\n\n\nextract_id\n\n extract_id (string)\n\n\nsource\n\n\nprocess_path\n\n process_path (name)\n\nMain dataloader functions:\n\nsource\n\n\nget_supervised_isic_test_dls\n\n get_supervised_isic_test_dls (bs, size, device, pct_dataset=1.0,\n                               num_workers=12)\n\n\nsource\n\n\nget_supervised_isic_train_dls\n\n get_supervised_isic_train_dls (bs, size, device, pct_dataset=1.0,\n                                num_workers=12)",
    "crumbs": [
      "isic_dataloading"
    ]
  }
]